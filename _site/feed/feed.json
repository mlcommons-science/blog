{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "MLCommons Science",
	"language": "en",
	"home_page_url": "https://mlcommons-science.netlify.app/",
	"feed_url": "https://mlcommons-science.netlify.app/feed/feed.json",
	"description": "The MLCommons Science Blog.",
	"author": {
		"name": "MLCommons",
		"url": "https://github.com/mlcommons/science"
	},
	"items": [
		{
			"id": "https://mlcommons-science.netlify.app/blog/earthquake-1/",
			"url": "https://mlcommons-science.netlify.app/blog/earthquake-1/",
			"title": "Improving the Earthquake Nowcasting Code",
			"content_html": "<h1 id=\"improving-the-earthquake-nowcasting-code\" tabindex=\"-1\">Improving the Earthquake Nowcasting Code <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/earthquake-1/\">#</a></h1>\n<p><em>Comments by Geoffrey Fox, 15 March 2023</em></p>\n<p>The Earthquake forecasting code has been improved following the recent\nstudies of</p>\n<ul>\n<li>\n<p>&quot;Does the Catalog of California Earthquakes, With Aftershocks\nIncluded, Contain Information About Future Large Earthquakes?&quot;  John\nB. Rundle, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, James\nCrutchfield, 10 February 2023\n<a href=\"https://doi.org/10.1029/2022EA002521\">https://doi.org/10.1029/2022EA002521</a></p>\n</li>\n<li>\n<p>&quot;Optimizing Earthquake Nowcasting With Machine Learning: The Role of\nStrain Hardening in the Earthquake Cycle,&quot; John B. Rundle, Joe\nYazbeck, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, Michael\nHeflin, James Crutchfield, 17 October 2022\n<a href=\"https://doi.org/10.1029/2022EA002343\">https://doi.org/10.1029/2022EA002343</a></p>\n</li>\n</ul>\n<p>These two papers identify that the occurrence rate of medium earthquakes\n(m <code>&gt;</code> 3.29) represented by smoothing in a time series of their number\nreveals the hidden variables controlling large earthquakes with\nmagnitude <code>&gt;=</code> 6.75. In particular, the rate of these medium earthquakes\ndecreases before a large quake; due to aftershocks, their number peaks\nafter a large earthquake.</p>\n<p>We added this observable with 9 different smoothing methods to the\nexisting earthquake nowcasting code with the simplest LSTM model. This\ngave for the earthquake activity in the next 4 years.</p>\n<ul>\n<li>\n<p>Normalized Nash-Sutcliffe Efficiency NNSE with 9 &quot;Physics Suggested&quot;\nData Training 0.948 Validation 0.866</p>\n</li>\n<li>\n<p>Normalized Nash-Sutcliffe Efficiency NNSE with Original code as in\nMLCommons Training 0.928 Validation 0.796</p>\n</li>\n</ul>\n<p>Which is a significant improvement. We used the Morris method to find\nwhich physics observable was most significant and ran with just this\ngetting slightly better.</p>\n<ul>\n<li>Normalized Nash-Sutcliffe Efficiency NNSE with Best &quot;Physics\nSuggested&quot; Data Training 0.956 Validation 0.866</li>\n</ul>\n<p>The time-dependent four-year predictions for the last fit is</p>\n<p><img src=\"https://mlcommons-science.netlify.app/image1.png\" alt=\"\"></p>\n<p>While the original code gives</p>\n<p><img src=\"https://mlcommons-science.netlify.app/image2.png\" alt=\"\"></p>\n",
			"date_published": "2023-03-16T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"url": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"title": "Cloudmask Improvements for Parallel Executions",
			"content_html": "<p>The original cloudmask code contains a feature to save the training\nmodel into a file that is later used for inference. However, at the\ncurrent time, the configuration file and the associated code save this\nconfiguration file in a single location. If run in parallel the model\nwould be overwritten among parallel runs. To allow parallel execution\nof cloudmask a mechanism to run the code on a permutation of different\nparameters is used. For this we can reuse <code>cloudmesh-sbatch</code> which\nallows the creation of a number of subdirectories that contain a\nmodified <code>config.yaml</code> file, as well as a custom-created batch\nscript. We also have modified the original python code to take into\nconsideration the new parameters in the YAML file.  This framework can\nbe adapted to various applications and HPC computers on which we\nexecute cloudmask.</p>\n<p>For more information, please contact Gregor von Laszewski\n<a href=\"mailto:laszewski@gmail.com\">laszewski@gmail.com</a></p>\n<p>Code for cloudmesh-sbatch is available on\n<a href=\"https://pypi.org/project/cloudmesh-sbatch/\">GitHub and PyPi</a>.</p>\n",
			"date_published": "2023-03-13T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"url": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"title": "Revised Goal Statement",
			"content_html": "<p>The goal of the MLCommons Research Science working group is to produce\nartifacts that help advance science. These artifacts are structured as\nbenchmarks with datasets, reference model(s), and goals that cover\nScience Research and Education. The primary goal is that our\nbenchmarks inspire research that achieves better scientific discovery\naccuracy .through our benchmarks. However, training students by using\nour artifacts in classes is also important. In this case, you can\nsubmit either a report on the class or class notes that use our\nartifacts to train the new scientists who will advance\ndiscovery. Unlike other MLCommons groups, we do not have a closed\ndivision but just the Research and Education open division that can\naccept multiple types of submissions. We will index all submissions\nand group them together into types of submissions. We only maintain a\nleaderboard for scientific accuracy submissions. Note in some\ninference examples, system performance needs to be improved to advance\nscience directly. Examples here include deep learning models for\nmicrostructure in fluid calculations that must be invoked at every\ngrid point. Another example is online data analysis to select\ninteresting events in real time. Our GitHub is used to submit\nartifacts for each submission, as in MLPerf training and\ninference. But we also offer a wiki-like interface for you to describe\nyour submissions. Sometimes you may only need the latter if, for\nexample, you are describing a class use of our benchmark artifacts\nwithout modification. The working group insists that all submissions\nare properly described with metadata so we can help advance this\naccessibility agenda.</p>\n",
			"date_published": "2023-03-12T00:00:00Z"
		}
		
	]
}
