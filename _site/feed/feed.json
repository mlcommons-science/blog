{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "MLCommons Science",
	"language": "en",
	"home_page_url": "https://mlcommons-science.netlify.app/",
	"feed_url": "https://mlcommons-science.netlify.app/feed/feed.json",
	"description": "The MLCommons Science Blog.",
	"author": {
		"name": "MLCommons",
		"url": "https://github.com/mlcommons/science"
	},
	"items": [
		{
			"id": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"url": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"title": "Cloudmask Improvements for Parallel Executions",
			"content_html": "<p>The original cloudmask code contains a feature to save the training\nmodel into a file that is later used for inference. However, at the\ncurrent time, the configuration file and the associated code save this\nconfiguration file in a single location. If run in parallel the model\nwould be overwritten among parallel runs. To allow parallel execution\nof cloudmask a mechanism to run the code on a permutation of different\nparameters is used. For this we can reuse <code>cloudmesh-sbatch</code> which\nallows the creation of a number of subdirectories that contain a\nmodified <code>config.yaml</code> file, as well as a custom-created batch\nscript. We also have modified the original python code to take into\nconsideration the new parameters in the YAML file.  This framework can\nbe adapted to various applications and HPC computers on which we\nexecute cloudmask.</p>\n<p>For more information, please contact Gregor von Laszewski\n<a href=\"mailto:laszewski@gmail.com\">laszewski@gmail.com</a></p>\n<p>Code for cloudmesh-sbatch is available on\n<a href=\"https://pypi.org/project/cloudmesh-sbatch/\">GitHub and PyPi</a>.</p>\n",
			"date_published": "2023-03-13T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"url": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"title": "Revised Goal Statement",
			"content_html": "<p>The goal of the MLCommons Research Science working group is to produce\nartifacts that help advance science. These artifacts are structured as\nbenchmarks with datasets, reference model(s), and goals that cover\nScience Research and Education. The primary goal is that our\nbenchmarks inspire research that achieves better scientific discovery\naccuracy .through our benchmarks. However, training students by using\nour artifacts in classes is also important. In this case, you can\nsubmit either a report on the class or class notes that use our\nartifacts to train the new scientists who will advance\ndiscovery. Unlike other MLCommons groups, we do not have a closed\ndivision but just the Research and Education open division that can\naccept multiple types of submissions. We will index all submissions\nand group them together into types of submissions. We only maintain a\nleaderboard for scientific accuracy submissions. Note in some\ninference examples, system performance needs to be improved to advance\nscience directly. Examples here include deep learning models for\nmicrostructure in fluid calculations that must be invoked at every\ngrid point. Another example is online data analysis to select\ninteresting events in real time. Our GitHub is used to submit\nartifacts for each submission, as in MLPerf training and\ninference. But we also offer a wiki-like interface for you to describe\nyour submissions. Sometimes you may only need the latter if, for\nexample, you are describing a class use of our benchmark artifacts\nwithout modification. The working group insists that all submissions\nare properly described with metadata so we can help advance this\naccessibility agenda.</p>\n",
			"date_published": "2023-03-12T00:00:00Z"
		}
		
	]
}
