<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
	<title>MLCommons Science</title>
	<subtitle>The MLCommons Science Blog.</subtitle>
	<link href="https://mlcommons-science.netlify.app/feed/feed.xml" rel="self"/>
	<link href="https://mlcommons-science.netlify.app/"/>
	<updated>2023-03-13T00:00:00Z</updated>
	<id>https://mlcommons-science.netlify.app/</id>
	<author>
		<name>MLCommons</name>
		<email>laszewski@gmail.com</email>
	</author>
	
	<entry>
		<title>Cloudmask Improvements for Parallel Executions</title>
		<link href="https://mlcommons-science.netlify.app/blog/cloudmask-1/"/>
		<updated>2023-03-13T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/cloudmask-1/</id>
		<content type="html">&lt;p&gt;The original cloudmask code contains a feature to save the training
model into a file that is later used for inference. However, at the
current time, the configuration file and the associated code save this
configuration file in a single location. If run in parallel the model
would be overwritten among parallel runs. To allow parallel execution
of cloudmask a mechanism to run the code on a permutation of different
parameters is used. For this we can reuse &lt;code&gt;cloudmesh-sbatch&lt;/code&gt; which
allows the creation of a number of subdirectories that contain a
modified &lt;code&gt;config.yaml&lt;/code&gt; file, as well as a custom-created batch
script. We also have modified the original python code to take into
consideration the new parameters in the YAML file.  This framework can
be adapted to various applications and HPC computers on which we
execute cloudmask.&lt;/p&gt;
&lt;p&gt;For more information, please contact Gregor von Laszewski
&lt;a href=&quot;mailto:laszewski@gmail.com&quot;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code for cloudmesh-sbatch is available on
&lt;a href=&quot;https://pypi.org/project/cloudmesh-sbatch/&quot;&gt;GitHub and PyPi&lt;/a&gt;.&lt;/p&gt;
</content>
	</entry>
</feed>
