<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
	<title>MLCommons Science</title>
	<subtitle>The MLCommons Science Blog.</subtitle>
	<link href="https://mlcommons-science.netlify.app/feed/feed.xml" rel="self"/>
	<link href="https://mlcommons-science.netlify.app/"/>
	<updated>2023-05-03T00:00:00Z</updated>
	<id>https://mlcommons-science.netlify.app/</id>
	<author>
		<name>MLCommons</name>
		<email>laszewski@gmail.com</email>
	</author>
	
	<entry>
		<title>MLCommons Machine Learning Benchmarks for Scientific Discovery</title>
		<link href="https://mlcommons-science.netlify.app/blog/science-about/"/>
		<updated>2023-05-03T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/science-about/</id>
		<content type="html">&lt;h2 id=&quot;mlcommons-machine-learning-benchmarks-for-scientific-discovery&quot; tabindex=&quot;-1&quot;&gt;MLCommons Machine Learning Benchmarks for Scientific Discovery &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/science-about/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Authors: Geoffrey Fox, Jeyan Thiyagalingam, Tony Hey, Juri Papay, Gregor von Laszewski and members of the MLCommons Science Working group&lt;/p&gt;
&lt;p&gt;The
&lt;a href=&quot;https://mlcommons.org/en/groups/research-science/&quot;&gt;MLCommonsÂ® Science working group&lt;/a&gt;
is pleased to announce the availability of
&lt;a href=&quot;https://github.com/mlcommons/science/tree/main/benchmarks&quot;&gt;MLCommons Science GitHub&lt;/a&gt;,
a series of 4 open source benchmarks. As a new addition to the
MLCommons suite of benchmarks, these tools are aimed at domain
scientists, machine learning experts, and students. The goal of
MLCommons Science GitHub is to uncover and gather novel Machine
Learning (ML) solutions tol improve scientific discovery by improving
accuracy on critical problems. Each benchmark includes open datasets,
and reference models with algorithms and software. These rigorously
documented benchmarks will also be used as tutorials and courses and
run on new systems. .&lt;/p&gt;
&lt;h3 id=&quot;join-us&quot; tabindex=&quot;-1&quot;&gt;Join Us &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/science-about/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are calling on the international community to join us for an open
call for participation to help increase model accuracy and/or extend
uses with other datasets or scientific fields. The call for
participation will have rolling submissions described in the overall
&lt;a href=&quot;https://github.com/mlcommons/science/blob/main/policy.adoc&quot;&gt;policy&lt;/a&gt;
and
&lt;a href=&quot;https://github.com/mlcommons/science/blob/main/submit.adoc&quot;&gt;submissions&lt;/a&gt;
documents. Participants should share their findings on the &lt;a href=&quot;https://github.com/mlcommons/science/tree/main/results&quot;&gt;MLCommons
Science Results GitHub&lt;/a&gt; at
&lt;a href=&quot;https://github.com/mlcommons/science/tree/main/results&quot;&gt;https://github.com/mlcommons/science/tree/main/results&lt;/a&gt;. Ideal
submissions will include a trained network that exceeds the target
accuracy of one of the reference models for one of the benchmarks,
together with a description of the improvements made.&lt;/p&gt;
&lt;h3 id=&quot;our-work&quot; tabindex=&quot;-1&quot;&gt;Our Work &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/science-about/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The MLCommons Science working group is part of the MLCommons research
initiative which spans work in Algorithms, DataPerf, Dynabench,
Medical, Science, and Storage. It collaborates with industry and
research, with a shared mission to accelerate machine learning
innovation to benefit society. The Science benchmarks differ from the
flagship MLPerf benchmarks of MLCommons in terms of an emphasis on
education and the impact on scientific discovery rather than system
performance. New benchmarks are currently being considered. If you
wish to join the open collaboration to discuss this work, contribute
new challenges, or have any questions, please contact
&lt;a href=&quot;mailto:sciencewg@mlcommons.org&quot;&gt;sciencewg@mlcommons.org&lt;/a&gt; or to reach the entire group
&lt;a href=&quot;https://groups.google.com/a/mlcommons.org/g/science&quot;&gt;https://groups.google.com/a/mlcommons.org/g/science&lt;/a&gt;. The working group
meets every 2 weeks. Joining MLCommons is described at
&lt;a href=&quot;https://mlcommons.org/en/get-involved/&quot;&gt;https://mlcommons.org/en/get-involved/&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;about-mlcommons&quot; tabindex=&quot;-1&quot;&gt;About MLCommons &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/science-about/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;MLCommons is an open engineering consortium with a mission to benefit
society by accelerating innovation in machine learning. The foundation
for MLCommons began with the MLPerf benchmark in 2018, which rapidly
scaled as a set of industry metrics to measure machine learning
performance and promote transparency of machine learning techniques.
In collaboration with its 50+ founding partners - global technology
providers, academics and researchers, MLCommons is focused on
collaborative engineering work that builds tools for the entire
machine learning industry through benchmarks and metrics, public
datasets and best practices.&lt;/p&gt;
&lt;p&gt;For additional information on MLCommons and details on becoming a
Member or Affiliate of the organization, please visit
&lt;a href=&quot;http://mlcommons.org/&quot;&gt;http://mlcommons.org/&lt;/a&gt; and contact &lt;a href=&quot;mailto:participation@mlcommons.org&quot;&gt;participation@mlcommons.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Press Contact: &lt;br&gt;
Kelly Berschauer &lt;br&gt;
&lt;a href=&quot;mailto:press@mlcommons.org&quot;&gt;press@mlcommons.org&lt;/a&gt;&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Improving the Earthquake Nowcasting Code</title>
		<link href="https://mlcommons-science.netlify.app/blog/earthquake-1/"/>
		<updated>2023-03-16T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/earthquake-1/</id>
		<content type="html">&lt;h1 id=&quot;improving-the-earthquake-nowcasting-code&quot; tabindex=&quot;-1&quot;&gt;Improving the Earthquake Nowcasting Code &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/earthquake-1/&quot;&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Comments by Geoffrey Fox, 15 March 2023&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Earthquake forecasting code has been improved following the recent
studies of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Does the Catalog of California Earthquakes, With Aftershocks
Included, Contain Information About Future Large Earthquakes?&amp;quot;  John
B. Rundle, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, James
Crutchfield, 10 February 2023
&lt;a href=&quot;https://doi.org/10.1029/2022EA002521&quot;&gt;https://doi.org/10.1029/2022EA002521&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Optimizing Earthquake Nowcasting With Machine Learning: The Role of
Strain Hardening in the Earthquake Cycle,&amp;quot; John B. Rundle, Joe
Yazbeck, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, Michael
Heflin, James Crutchfield, 17 October 2022
&lt;a href=&quot;https://doi.org/10.1029/2022EA002343&quot;&gt;https://doi.org/10.1029/2022EA002343&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two papers identify that the occurrence rate of medium earthquakes
(m &lt;code&gt;&amp;gt;&lt;/code&gt; 3.29) represented by smoothing in a time series of their number
reveals the hidden variables controlling large earthquakes with
magnitude &lt;code&gt;&amp;gt;=&lt;/code&gt; 6.75. In particular, the rate of these medium earthquakes
decreases before a large quake; due to aftershocks, their number peaks
after a large earthquake.&lt;/p&gt;
&lt;p&gt;We added this observable with 9 different smoothing methods to the
existing earthquake nowcasting code with the simplest LSTM model. This
gave for the earthquake activity in the next 4 years.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Normalized Nash-Sutcliffe Efficiency NNSE with 9 &amp;quot;Physics Suggested&amp;quot;
Data Training 0.948 Validation 0.866&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normalized Nash-Sutcliffe Efficiency NNSE with Original code as in
MLCommons Training 0.928 Validation 0.796&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which is a significant improvement. We used the Morris method to find
which physics observable was most significant and ran with just this
getting slightly better.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normalized Nash-Sutcliffe Efficiency NNSE with Best &amp;quot;Physics
Suggested&amp;quot; Data Training 0.956 Validation 0.866&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The time-dependent four-year predictions for the last fit is&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/mlcommons-science/mlcommons-science.github.io/raw/main/docs/img/image1.png&quot; alt=&quot;Image1&quot;&gt;&lt;/p&gt;
&lt;p&gt;While the original code gives&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/mlcommons-science/mlcommons-science.github.io/raw/main/docs/img/image2.png&quot; alt=&quot;Image2&quot;&gt;&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Cloudmask Improvements for Parallel Executions</title>
		<link href="https://mlcommons-science.netlify.app/blog/cloudmask-1/"/>
		<updated>2023-03-13T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/cloudmask-1/</id>
		<content type="html">&lt;p&gt;The original cloudmask code contains a feature to save the training
model into a file that is later used for inference. However, at the
current time, the configuration file and the associated code save this
configuration file in a single location. If run in parallel the model
would be overwritten among parallel runs. To allow parallel execution
of cloudmask a mechanism to run the code on a permutation of different
parameters is used. For this we can reuse &lt;code&gt;cloudmesh-sbatch&lt;/code&gt; which
allows the creation of a number of subdirectories that contain a
modified &lt;code&gt;config.yaml&lt;/code&gt; file, as well as a custom-created batch
script. We also have modified the original python code to take into
consideration the new parameters in the YAML file.  This framework can
be adapted to various applications and HPC computers on which we
execute cloudmask.&lt;/p&gt;
&lt;p&gt;For more information, please contact Gregor von Laszewski
&lt;a href=&quot;mailto:laszewski@gmail.com&quot;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code for cloudmesh-sbatch is available on
&lt;a href=&quot;https://pypi.org/project/cloudmesh-sbatch/&quot;&gt;GitHub and PyPi&lt;/a&gt;.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Revised Goal Statement</title>
		<link href="https://mlcommons-science.netlify.app/blog/goal-1/"/>
		<updated>2023-03-12T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/goal-1/</id>
		<content type="html">&lt;p&gt;The goal of the MLCommons Research Science working group is to produce
artifacts that help advance science. These artifacts are structured as
benchmarks with datasets, reference model(s), and goals that cover
Science Research and Education. The primary goal is that our
benchmarks inspire research that achieves better scientific discovery
through our benchmarks. However, training students by using
our artifacts in classes is also important. In this case, you can
submit either a report on the class or class notes that use our
artifacts to train the new scientists who will advance
discovery. Unlike other MLCommons groups, we do not have a closed
division but just the Research and Education open division that can
accept multiple types of submissions. We will index all submissions
and group them together into types of submissions. We only maintain a
leaderboard for scientific accuracy submissions. Note in some
inference examples, system performance needs to be improved to advance
science directly. Examples here include deep learning models for
microstructure in fluid calculations that must be invoked at every
grid point. Another example is online data analysis to select
interesting events in real time. Our GitHub is used to submit
artifacts for each submission, as in MLPerf training and
inference. But we also offer a wiki-like interface for you to describe
your submissions. Sometimes you may only need the latter if, for
example, you are describing a class use of our benchmark artifacts
without modification. The working group insists that all submissions
are properly described with metadata so we can help advance this
accessibility agenda.&lt;/p&gt;
</content>
	</entry>
</feed>
