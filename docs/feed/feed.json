{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "MLCommons Science",
	"language": "en",
	"home_page_url": "https://mlcommons-science.netlify.app/",
	"feed_url": "https://mlcommons-science.netlify.app/feed/feed.json",
	"description": "The MLCommons Science Blog.",
	"author": {
		"name": "MLCommons",
		"url": "https://github.com/mlcommons/science"
	},
	"items": [
		{
			"id": "https://mlcommons-science.netlify.app/blog/science-about/",
			"url": "https://mlcommons-science.netlify.app/blog/science-about/",
			"title": "MLCommons Machine Learning Benchmarks for Scientific Discovery",
			"content_html": "<h2 id=\"mlcommons-machine-learning-benchmarks-for-scientific-discovery\" tabindex=\"-1\">MLCommons Machine Learning Benchmarks for Scientific Discovery <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/science-about/\">#</a></h2>\n<p>Authors: Geoffrey Fox, Jeyan Thiyagalingam, Tony Hey, Juri Papay, Gregor von Laszewski and members of the MLCommons Science Working group</p>\n<p>The\n<a href=\"https://mlcommons.org/en/groups/research-science/\">MLCommonsÂ® Science working group</a>\nis pleased to announce the availability of\n<a href=\"https://github.com/mlcommons/science/tree/main/benchmarks\">MLCommons Science GitHub</a>,\na series of 4 open source benchmarks. As a new addition to the\nMLCommons suite of benchmarks, these tools are aimed at domain\nscientists, machine learning experts, and students. The goal of\nMLCommons Science GitHub is to uncover and gather novel Machine\nLearning (ML) solutions tol improve scientific discovery by improving\naccuracy on critical problems. Each benchmark includes open datasets,\nand reference models with algorithms and software. These rigorously\ndocumented benchmarks will also be used as tutorials and courses and\nrun on new systems. .</p>\n<h3 id=\"join-us\" tabindex=\"-1\">Join Us <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/science-about/\">#</a></h3>\n<p>We are calling on the international community to join us for an open\ncall for participation to help increase model accuracy and/or extend\nuses with other datasets or scientific fields. The call for\nparticipation will have rolling submissions described in the overall\n<a href=\"https://github.com/mlcommons/science/blob/main/policy.adoc\">policy</a>\nand\n<a href=\"https://github.com/mlcommons/science/blob/main/submit.adoc\">submissions</a>\ndocuments. Participants should share their findings on the <a href=\"https://github.com/mlcommons/science/tree/main/results\">MLCommons\nScience Results GitHub</a> at\n<a href=\"https://github.com/mlcommons/science/tree/main/results\">https://github.com/mlcommons/science/tree/main/results</a>. Ideal\nsubmissions will include a trained network that exceeds the target\naccuracy of one of the reference models for one of the benchmarks,\ntogether with a description of the improvements made.</p>\n<h3 id=\"our-work\" tabindex=\"-1\">Our Work <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/science-about/\">#</a></h3>\n<p>The MLCommons Science working group is part of the MLCommons research\ninitiative which spans work in Algorithms, DataPerf, Dynabench,\nMedical, Science, and Storage. It collaborates with industry and\nresearch, with a shared mission to accelerate machine learning\ninnovation to benefit society. The Science benchmarks differ from the\nflagship MLPerf benchmarks of MLCommons in terms of an emphasis on\neducation and the impact on scientific discovery rather than system\nperformance. New benchmarks are currently being considered. If you\nwish to join the open collaboration to discuss this work, contribute\nnew challenges, or have any questions, please contact\n<a href=\"mailto:sciencewg@mlcommons.org\">sciencewg@mlcommons.org</a> or to reach the entire group\n<a href=\"https://groups.google.com/a/mlcommons.org/g/science\">https://groups.google.com/a/mlcommons.org/g/science</a>. The working group\nmeets every 2 weeks. Joining MLCommons is described at\n<a href=\"https://mlcommons.org/en/get-involved/\">https://mlcommons.org/en/get-involved/</a>.</p>\n<h3 id=\"about-mlcommons\" tabindex=\"-1\">About MLCommons <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/science-about/\">#</a></h3>\n<p>MLCommons is an open engineering consortium with a mission to benefit\nsociety by accelerating innovation in machine learning. The foundation\nfor MLCommons began with the MLPerf benchmark in 2018, which rapidly\nscaled as a set of industry metrics to measure machine learning\nperformance and promote transparency of machine learning techniques.\nIn collaboration with its 50+ founding partners - global technology\nproviders, academics and researchers, MLCommons is focused on\ncollaborative engineering work that builds tools for the entire\nmachine learning industry through benchmarks and metrics, public\ndatasets and best practices.</p>\n<p>For additional information on MLCommons and details on becoming a\nMember or Affiliate of the organization, please visit\n<a href=\"http://mlcommons.org/\">http://mlcommons.org/</a> and contact <a href=\"mailto:participation@mlcommons.org\">participation@mlcommons.org</a>.</p>\n<p>Press Contact: <br>\nKelly Berschauer <br>\n<a href=\"mailto:press@mlcommons.org\">press@mlcommons.org</a></p>\n",
			"date_published": "2023-05-03T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/earthquake-1/",
			"url": "https://mlcommons-science.netlify.app/blog/earthquake-1/",
			"title": "Improving the Earthquake Nowcasting Code",
			"content_html": "<h1 id=\"improving-the-earthquake-nowcasting-code\" tabindex=\"-1\">Improving the Earthquake Nowcasting Code <a class=\"header-anchor\" href=\"https://mlcommons-science.netlify.app/blog/earthquake-1/\">#</a></h1>\n<p><em>Comments by Geoffrey Fox, 15 March 2023</em></p>\n<p>The Earthquake forecasting code has been improved following the recent\nstudies of</p>\n<ul>\n<li>\n<p>&quot;Does the Catalog of California Earthquakes, With Aftershocks\nIncluded, Contain Information About Future Large Earthquakes?&quot;  John\nB. Rundle, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, James\nCrutchfield, 10 February 2023\n<a href=\"https://doi.org/10.1029/2022EA002521\">https://doi.org/10.1029/2022EA002521</a></p>\n</li>\n<li>\n<p>&quot;Optimizing Earthquake Nowcasting With Machine Learning: The Role of\nStrain Hardening in the Earthquake Cycle,&quot; John B. Rundle, Joe\nYazbeck, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, Michael\nHeflin, James Crutchfield, 17 October 2022\n<a href=\"https://doi.org/10.1029/2022EA002343\">https://doi.org/10.1029/2022EA002343</a></p>\n</li>\n</ul>\n<p>These two papers identify that the occurrence rate of medium earthquakes\n(m <code>&gt;</code> 3.29) represented by smoothing in a time series of their number\nreveals the hidden variables controlling large earthquakes with\nmagnitude <code>&gt;=</code> 6.75. In particular, the rate of these medium earthquakes\ndecreases before a large quake; due to aftershocks, their number peaks\nafter a large earthquake.</p>\n<p>We added this observable with 9 different smoothing methods to the\nexisting earthquake nowcasting code with the simplest LSTM model. This\ngave for the earthquake activity in the next 4 years.</p>\n<ul>\n<li>\n<p>Normalized Nash-Sutcliffe Efficiency NNSE with 9 &quot;Physics Suggested&quot;\nData Training 0.948 Validation 0.866</p>\n</li>\n<li>\n<p>Normalized Nash-Sutcliffe Efficiency NNSE with Original code as in\nMLCommons Training 0.928 Validation 0.796</p>\n</li>\n</ul>\n<p>Which is a significant improvement. We used the Morris method to find\nwhich physics observable was most significant and ran with just this\ngetting slightly better.</p>\n<ul>\n<li>Normalized Nash-Sutcliffe Efficiency NNSE with Best &quot;Physics\nSuggested&quot; Data Training 0.956 Validation 0.866</li>\n</ul>\n<p>The time-dependent four-year predictions for the last fit is</p>\n<p><img src=\"https://github.com/mlcommons-science/mlcommons-science.github.io/raw/main/docs/img/image1.png\" alt=\"Image1\"></p>\n<p>While the original code gives</p>\n<p><img src=\"https://github.com/mlcommons-science/mlcommons-science.github.io/raw/main/docs/img/image2.png\" alt=\"Image2\"></p>\n",
			"date_published": "2023-03-16T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"url": "https://mlcommons-science.netlify.app/blog/cloudmask-1/",
			"title": "Cloudmask Improvements for Parallel Executions",
			"content_html": "<p>The original cloudmask code contains a feature to save the training\nmodel into a file that is later used for inference. However, at the\ncurrent time, the configuration file and the associated code save this\nconfiguration file in a single location. If run in parallel the model\nwould be overwritten among parallel runs. To allow parallel execution\nof cloudmask a mechanism to run the code on a permutation of different\nparameters is used. For this we can reuse <code>cloudmesh-sbatch</code> which\nallows the creation of a number of subdirectories that contain a\nmodified <code>config.yaml</code> file, as well as a custom-created batch\nscript. We also have modified the original python code to take into\nconsideration the new parameters in the YAML file.  This framework can\nbe adapted to various applications and HPC computers on which we\nexecute cloudmask.</p>\n<p>For more information, please contact Gregor von Laszewski\n<a href=\"mailto:laszewski@gmail.com\">laszewski@gmail.com</a></p>\n<p>Code for cloudmesh-sbatch is available on\n<a href=\"https://pypi.org/project/cloudmesh-sbatch/\">GitHub and PyPi</a>.</p>\n",
			"date_published": "2023-03-13T00:00:00Z"
		}
		,
		{
			"id": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"url": "https://mlcommons-science.netlify.app/blog/goal-1/",
			"title": "Revised Goal Statement",
			"content_html": "<p>The goal of the MLCommons Research Science working group is to produce\nartifacts that help advance science. These artifacts are structured as\nbenchmarks with datasets, reference model(s), and goals that cover\nScience Research and Education. The primary goal is that our\nbenchmarks inspire research that achieves better scientific discovery\nthrough our benchmarks. However, training students by using\nour artifacts in classes is also important. In this case, you can\nsubmit either a report on the class or class notes that use our\nartifacts to train the new scientists who will advance\ndiscovery. Unlike other MLCommons groups, we do not have a closed\ndivision but just the Research and Education open division that can\naccept multiple types of submissions. We will index all submissions\nand group them together into types of submissions. We only maintain a\nleaderboard for scientific accuracy submissions. Note in some\ninference examples, system performance needs to be improved to advance\nscience directly. Examples here include deep learning models for\nmicrostructure in fluid calculations that must be invoked at every\ngrid point. Another example is online data analysis to select\ninteresting events in real time. Our GitHub is used to submit\nartifacts for each submission, as in MLPerf training and\ninference. But we also offer a wiki-like interface for you to describe\nyour submissions. Sometimes you may only need the latter if, for\nexample, you are describing a class use of our benchmark artifacts\nwithout modification. The working group insists that all submissions\nare properly described with metadata so we can help advance this\naccessibility agenda.</p>\n",
			"date_published": "2023-03-12T00:00:00Z"
		}
		
	]
}
