<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
	<title>MLCommons Science</title>
	<subtitle>The MLCommons Science Blog.</subtitle>
	<link href="https://mlcommons-science.netlify.app/feed/feed.xml" rel="self"/>
	<link href="https://mlcommons-science.netlify.app/"/>
	<updated>2023-03-16T00:00:00Z</updated>
	<id>https://mlcommons-science.netlify.app/</id>
	<author>
		<name>MLCommons</name>
		<email>laszewski@gmail.com</email>
	</author>
	
	<entry>
		<title>Improving the Earthquake Nowcasting Code</title>
		<link href="https://mlcommons-science.netlify.app/blog/earthquake-1/"/>
		<updated>2023-03-16T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/earthquake-1/</id>
		<content type="html">&lt;h1 id=&quot;improving-the-earthquake-nowcasting-code&quot; tabindex=&quot;-1&quot;&gt;Improving the Earthquake Nowcasting Code &lt;a class=&quot;header-anchor&quot; href=&quot;https://mlcommons-science.netlify.app/blog/earthquake-1/&quot;&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Comments by Geoffrey Fox, 15 March 2023&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Earthquake forecasting code has been improved following the recent
studies of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Does the Catalog of California Earthquakes, With Aftershocks
Included, Contain Information About Future Large Earthquakes?&amp;quot;  John
B. Rundle, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, James
Crutchfield, 10 February 2023
&lt;a href=&quot;https://doi.org/10.1029/2022EA002521&quot;&gt;https://doi.org/10.1029/2022EA002521&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot;Optimizing Earthquake Nowcasting With Machine Learning: The Role of
Strain Hardening in the Earthquake Cycle,&amp;quot; John B. Rundle, Joe
Yazbeck, Andrea Donnellan, Geoffrey Fox, Lisa Grant Ludwig, Michael
Heflin, James Crutchfield, 17 October 2022
&lt;a href=&quot;https://doi.org/10.1029/2022EA002343&quot;&gt;https://doi.org/10.1029/2022EA002343&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two papers identify that the occurrence rate of medium earthquakes
(m &lt;code&gt;&amp;gt;&lt;/code&gt; 3.29) represented by smoothing in a time series of their number
reveals the hidden variables controlling large earthquakes with
magnitude &lt;code&gt;&amp;gt;=&lt;/code&gt; 6.75. In particular, the rate of these medium earthquakes
decreases before a large quake; due to aftershocks, their number peaks
after a large earthquake.&lt;/p&gt;
&lt;p&gt;We added this observable with 9 different smoothing methods to the
existing earthquake nowcasting code with the simplest LSTM model. This
gave for the earthquake activity in the next 4 years.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Normalized Nash-Sutcliffe Efficiency NNSE with 9 &amp;quot;Physics Suggested&amp;quot;
Data Training 0.948 Validation 0.866&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normalized Nash-Sutcliffe Efficiency NNSE with Original code as in
MLCommons Training 0.928 Validation 0.796&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which is a significant improvement. We used the Morris method to find
which physics observable was most significant and ran with just this
getting slightly better.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normalized Nash-Sutcliffe Efficiency NNSE with Best &amp;quot;Physics
Suggested&amp;quot; Data Training 0.956 Validation 0.866&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The time-dependent four-year predictions for the last fit is&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mlcommons-science.netlify.app/blog/earthquake-1/image1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;While the original code gives&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mlcommons-science.netlify.app/blog/earthquake-1/image2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Cloudmask Improvements for Parallel Executions</title>
		<link href="https://mlcommons-science.netlify.app/blog/cloudmask-1/"/>
		<updated>2023-03-13T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/cloudmask-1/</id>
		<content type="html">&lt;p&gt;The original cloudmask code contains a feature to save the training
model into a file that is later used for inference. However, at the
current time, the configuration file and the associated code save this
configuration file in a single location. If run in parallel the model
would be overwritten among parallel runs. To allow parallel execution
of cloudmask a mechanism to run the code on a permutation of different
parameters is used. For this we can reuse &lt;code&gt;cloudmesh-sbatch&lt;/code&gt; which
allows the creation of a number of subdirectories that contain a
modified &lt;code&gt;config.yaml&lt;/code&gt; file, as well as a custom-created batch
script. We also have modified the original python code to take into
consideration the new parameters in the YAML file.  This framework can
be adapted to various applications and HPC computers on which we
execute cloudmask.&lt;/p&gt;
&lt;p&gt;For more information, please contact Gregor von Laszewski
&lt;a href=&quot;mailto:laszewski@gmail.com&quot;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code for cloudmesh-sbatch is available on
&lt;a href=&quot;https://pypi.org/project/cloudmesh-sbatch/&quot;&gt;GitHub and PyPi&lt;/a&gt;.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Revised Goal Statement</title>
		<link href="https://mlcommons-science.netlify.app/blog/goal-1/"/>
		<updated>2023-03-12T00:00:00Z</updated>
		<id>https://mlcommons-science.netlify.app/blog/goal-1/</id>
		<content type="html">&lt;p&gt;The goal of the MLCommons Research Science working group is to produce
artifacts that help advance science. These artifacts are structured as
benchmarks with datasets, reference model(s), and goals that cover
Science Research and Education. The primary goal is that our
benchmarks inspire research that achieves better scientific discovery
accuracy .through our benchmarks. However, training students by using
our artifacts in classes is also important. In this case, you can
submit either a report on the class or class notes that use our
artifacts to train the new scientists who will advance
discovery. Unlike other MLCommons groups, we do not have a closed
division but just the Research and Education open division that can
accept multiple types of submissions. We will index all submissions
and group them together into types of submissions. We only maintain a
leaderboard for scientific accuracy submissions. Note in some
inference examples, system performance needs to be improved to advance
science directly. Examples here include deep learning models for
microstructure in fluid calculations that must be invoked at every
grid point. Another example is online data analysis to select
interesting events in real time. Our GitHub is used to submit
artifacts for each submission, as in MLPerf training and
inference. But we also offer a wiki-like interface for you to describe
your submissions. Sometimes you may only need the latter if, for
example, you are describing a class use of our benchmark artifacts
without modification. The working group insists that all submissions
are properly described with metadata so we can help advance this
accessibility agenda.&lt;/p&gt;
</content>
	</entry>
</feed>
